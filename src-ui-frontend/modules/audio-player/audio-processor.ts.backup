// Audio processing and Web Audio API functionality

export interface CaptureResult {
  frequencies: number[];
  magnitudes: number[];
  phases: number[]; // Phase data in degrees
  success: boolean;
  error?: string;
}

// DEPRECATED: This class has been replaced by CamillaDSP backend integration.
// Audio capture features have been removed as they relied on WebRTC.
// Use the new CamillaAudioManager for audio playback.
export class AudioProcessor {
  // Stub class - all functionality moved to backend
  private capturing: boolean = false;

  constructor() {
    console.warn("AudioProcessor is deprecated. Use CamillaAudioManager instead.");
  }


  // REMOVED: WebRTC audio capture functionality
  // Use backend audio_start_recording() command for recording features
  private createNoiseSource(type: "white" | "pink"): never {
    throw new Error("Audio capture has been removed. Use backend CamillaDSP recording instead.");
    const buffer = this.audioContext.createBuffer(
      1,
      bufferSize,
      this.audioContext.sampleRate,
    );
    const output = buffer.getChannelData(0);

    if (type === "white") {
      // White noise: random values
      for (let i = 0; i < bufferSize; i++) {
        output[i] = Math.random() * 2 - 1;
      }
    } else if (type === "pink") {
      // Pink noise using Paul Kellet's algorithm
      let b0 = 0,
        b1 = 0,
        b2 = 0,
        b3 = 0,
        b4 = 0,
        b5 = 0,
        b6 = 0;
      for (let i = 0; i < bufferSize; i++) {
        const white = Math.random() * 2 - 1;
        b0 = 0.99886 * b0 + white * 0.0555179;
        b1 = 0.99332 * b1 + white * 0.0750759;
        b2 = 0.969 * b2 + white * 0.153852;
        b3 = 0.8665 * b3 + white * 0.3104856;
        b4 = 0.55 * b4 + white * 0.5329522;
        b5 = -0.7616 * b5 - white * 0.016898;
        output[i] = (b0 + b1 + b2 + b3 + b4 + b5 + b6 + white * 0.5362) * 0.11;
        b6 = white * 0.115926;
      }
    }

    this.noiseSource = this.audioContext.createBufferSource();
    this.noiseSource.buffer = buffer;
    this.noiseBuffer = buffer;

    return this.noiseSource;
  }

  async loadAudioFile(file: File): Promise<void> {
    if (!this.audioContext) {
      throw new Error("Audio context not initialized");
    }

    try {
      const arrayBuffer = await file.arrayBuffer();
      this.audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      console.log("Audio file loaded successfully");

      // Update audio status elements if available
      this.updateAudioStatus();
    } catch (error) {
      console.error("Error loading audio file:", error);
      throw error;
    }
  }

  async loadAudioFromUrl(url: string): Promise<void> {
    if (!this.audioContext) {
      throw new Error("Audio context not initialized");
    }

    try {
      console.log("Loading audio from URL:", url);
      const response = await fetch(url);

      if (!response.ok) {
        throw new Error(
          `Failed to fetch audio: ${response.status} ${response.statusText}`,
        );
      }

      const arrayBuffer = await response.arrayBuffer();
      console.log("Audio data fetched, decoding...");

      this.audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      console.log("Audio loaded from URL successfully:", {
        duration: this.audioBuffer.duration,
        sampleRate: this.audioBuffer.sampleRate,
        channels: this.audioBuffer.numberOfChannels,
      });

      // Update audio status elements if available
      this.updateAudioStatus();
    } catch (error) {
      console.error("Error loading audio from URL:", error);
      throw error;
    }
  }

  updateFilterParams(filterParams: number[]): void {
    this.currentFilterParams = [...filterParams];
    this.setupEQFilters();
  }

  private setupEQFilters(): void {
    if (!this.audioContext || !this.gainNode) return;

    // Clear existing filters
    this.eqFilters.forEach((filter) => filter.disconnect());
    this.eqFilters = [];

    // Create new filters from parameters
    for (let i = 0; i < this.currentFilterParams.length; i += 3) {
      if (i + 2 < this.currentFilterParams.length) {
        const freq = this.currentFilterParams[i];
        const q = this.currentFilterParams[i + 1];
        const gain = this.currentFilterParams[i + 2];

        if (Math.abs(gain) > 0.1) {
          // Only create filter if gain is significant
          const filter = this.audioContext.createBiquadFilter();
          filter.type = "peaking";
          filter.frequency.value = freq;
          filter.Q.value = q;
          filter.gain.value = gain;
          this.eqFilters.push(filter);
        }
      }
    }

    console.log(`Created ${this.eqFilters.length} EQ filters`);
  }

  private connectAudioChain(): void {
    if (!this.audioSource || !this.gainNode || !this.audioContext) {
      console.error("Cannot connect audio chain - missing components:", {
        audioSource: !!this.audioSource,
        gainNode: !!this.gainNode,
        audioContext: !!this.audioContext,
      });
      return;
    }

    console.log(
      "Connecting audio chain with",
      this.eqFilters.length,
      "EQ filters",
    );
    let currentNode: AudioNode = this.audioSource;

    // Connect EQ filters in series
    this.eqFilters.forEach((filter, index) => {
      console.log(`Connecting EQ filter ${index + 1}`);
      currentNode.connect(filter);
      currentNode = filter;
    });

    // Connect to gain and analyzer
    console.log("Connecting to gain node and destination");
    currentNode.connect(this.gainNode);
    if (this.analyserNode) {
      this.gainNode.connect(this.analyserNode);
      this.analyserNode.connect(this.audioContext.destination);
      console.log(
        "Audio chain connected: source -> EQ filters -> gain -> analyser -> destination",
      );
    } else {
      this.gainNode.connect(this.audioContext.destination);
      console.log(
        "Audio chain connected: source -> EQ filters -> gain -> destination",
      );
    }
  }

  setEQEnabled(enabled: boolean): void {
    this.eqEnabled = enabled;

    if (!this.gainNode) return;

    if (enabled && this.currentFilterParams.length > 0) {
      this.setupEQFilters();
    } else {
      // Disconnect all EQ filters but keep the audio chain connected
      this.eqFilters.forEach((filter) => filter.disconnect());
      this.eqFilters = [];

      // Reconnect audio chain without EQ filters
      if (this.audioSource && this.isAudioPlaying) {
        this.connectAudioChain();
      }
    }

    console.log(`EQ ${enabled ? "enabled" : "disabled"}`);
    this.updateAudioStatus();
  }

  getCurrentTime(): number {
    if (!this.audioContext || !this.isAudioPlaying) return 0;
    return this.audioContext.currentTime - this.audioStartTime;
  }

  getDuration(): number {
    return this.audioBuffer ? this.audioBuffer.duration : 0;
  }

  isPlaying(): boolean {
    return this.isAudioPlaying;
  }

  isCapturing(): boolean {
    return this.capturing;
  }

  setupSpectrumAnalyzer(canvas: HTMLCanvasElement): void {
    this.spectrumCanvas = canvas;
    this.spectrumCtx = canvas.getContext("2d");
    this.calculateFrequencyBinRanges();
  }

  private calculateFrequencyBinRanges(): void {
    if (!this.analyserNode || !this.audioContext) return;

    const nyquist = this.audioContext.sampleRate / 2;
    const binCount = this.analyserNode.frequencyBinCount;

    this.frequencyBinRanges = [];
    for (let i = 0; i < binCount; i++) {
      const freq = (i * nyquist) / binCount;
      this.frequencyBinRanges.push({
        start: freq,
        end: ((i + 1) * nyquist) / binCount,
      });
    }
  }

  startSpectrumAnalysis(): void {
    if (!this.analyserNode || !this.spectrumCanvas || !this.spectrumCtx) return;

    const dataArray = new Uint8Array(this.analyserNode.frequencyBinCount);

    const draw = () => {
      if (!this.analyserNode || !this.spectrumCanvas || !this.spectrumCtx)
        return;

      this.analyserNode.getByteFrequencyData(dataArray);

      const width = this.spectrumCanvas.width;
      const height = this.spectrumCanvas.height;

      this.spectrumCtx.fillStyle = "rgb(0, 0, 0)";
      this.spectrumCtx.fillRect(0, 0, width, height);

      const barWidth = width / dataArray.length;
      let x = 0;

      for (let i = 0; i < dataArray.length; i++) {
        const barHeight = (dataArray[i] / 255) * height;

        this.spectrumCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`;
        this.spectrumCtx.fillRect(x, height - barHeight, barWidth, barHeight);

        x += barWidth;
      }

      this.spectrumAnimationFrame = requestAnimationFrame(draw);
    };

    draw();
  }

  stopSpectrumAnalysis(): void {
    if (this.spectrumAnimationFrame) {
      cancelAnimationFrame(this.spectrumAnimationFrame);
      this.spectrumAnimationFrame = null;
    }
  }

  // Audio device enumeration
  async enumerateAudioDevices(): Promise<MediaDeviceInfo[]> {
    try {
      // Request permission first
      await navigator.mediaDevices
        .getUserMedia({ audio: true })
        .then((stream) => {
          // Immediately stop the stream after getting permission
          stream.getTracks().forEach((track) => track.stop());
        });

      // Now enumerate devices
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(
        (device) => device.kind === "audioinput",
      );
      console.log("Found audio input devices:", audioInputs);
      return audioInputs;
    } catch (error) {
      console.error("Error enumerating audio devices:", error);
      return [];
    }
  }

  // Audio output device enumeration
  async enumerateAudioOutputDevices(): Promise<MediaDeviceInfo[]> {
    try {
      // Request permission first (needed for device labels)
      await navigator.mediaDevices
        .getUserMedia({ audio: true })
        .then((stream) => {
          // Immediately stop the stream after getting permission
          stream.getTracks().forEach((track) => track.stop());
        });

      // Now enumerate devices
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioOutputs = devices.filter(
        (device) => device.kind === "audiooutput",
      );
      console.log("Found audio output devices:", audioOutputs);
      return audioOutputs;
    } catch (error) {
      console.error("Error enumerating audio output devices:", error);
      return [];
    }
  }

  // Audio capture functionality
  async startCapture(deviceId?: string): Promise<CaptureResult> {
    console.log("Starting audio capture with device:", deviceId || "default");
    console.log(
      "Sample rate:",
      this.captureSampleRate,
      "Signal type:",
      this.signalType,
    );

    if (this.capturing) {
      throw new Error("Capture already in progress");
    }

    try {
      // Check if capture is supported
      if (!this.isCaptureSupported()) {
        const error =
          "Microphone capture is not supported by your browser. Please ensure you are using a modern browser with WebRTC support.";
        console.error("Capture not supported:", error);
        return {
          frequencies: [],
          magnitudes: [],
          phases: [],
          success: false,
          error: error,
        };
      }

      // Recreate audio context with desired sample rate if needed
      if (
        !this.audioContext ||
        this.audioContext.sampleRate !== this.captureSampleRate
      ) {
        if (this.audioContext) {
          this.audioContext.close();
        }
        this.audioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({
          sampleRate: this.captureSampleRate,
        });
        console.log(
          "Created audio context with sample rate:",
          this.audioContext.sampleRate,
        );
      }

      // Request microphone access with specific device if provided
      let audioConstraints: MediaStreamConstraints = {
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: this.captureSampleRate,
          ...(deviceId && deviceId !== "default"
            ? { deviceId: { exact: deviceId } }
            : {}),
        },
      };

      console.log(
        "Requesting getUserMedia with constraints:",
        audioConstraints,
      );

      try {
        this.mediaStream =
          await navigator.mediaDevices.getUserMedia(audioConstraints);
      } catch (error: any) {
        // If we get OverconstrainedError with a specific device, try with default
        if (
          error.name === "OverconstrainedError" &&
          deviceId &&
          deviceId !== "default"
        ) {
          console.warn(
            `Failed to use device ${deviceId}, falling back to default device`,
          );
          audioConstraints = {
            audio: {
              echoCancellation: false,
              noiseSuppression: false,
              autoGainControl: false,
              sampleRate: this.captureSampleRate,
            },
          };
          this.mediaStream =
            await navigator.mediaDevices.getUserMedia(audioConstraints);
        } else {
          throw error;
        }
      }

      if (!this.audioContext) {
        throw new Error("Audio context not initialized");
      }

      // Create media stream source
      this.mediaStreamSource = this.audioContext.createMediaStreamSource(
        this.mediaStream,
      );

      // Create analyzer for capture
      this.captureAnalyser = this.audioContext.createAnalyser();
      this.captureAnalyser.fftSize = 8192;
      this.captureAnalyser.smoothingTimeConstant = 0.1;

      // Connect stream to analyzer
      this.mediaStreamSource.connect(this.captureAnalyser);

      this.capturing = true;
      this.captureController = new AbortController();

      // Perform the actual capture measurement
      const result = await this.performCaptureMeasurement();

      return result;
    } catch (error) {
      console.error("Error during audio capture:", error);
      this.stopCapture();

      // Provide more specific error messages based on the error type
      let errorMessage = "Unknown capture error";
      if (error instanceof Error) {
        if (error.name === "NotAllowedError") {
          errorMessage =
            "Microphone permission denied. Please allow microphone access and try again.";
        } else if (error.name === "NotFoundError") {
          errorMessage =
            "No microphone found. Please connect a microphone and try again.";
        } else if (error.name === "NotReadableError") {
          errorMessage = "Microphone is already in use by another application.";
        } else if (error.name === "OverconstrainedError") {
          errorMessage =
            "The selected microphone settings are not supported by your device.";
        } else {
          errorMessage = error.message;
        }
      }

      return {
        frequencies: [],
        magnitudes: [],
        phases: [],
        success: false,
        error: errorMessage,
      };
    }
  }

  stopCapture(): void {
    console.log("Stopping audio capture...");

    this.capturing = false;

    if (this.oscillator) {
      try {
        this.oscillator.stop();
        this.oscillator.disconnect();
      } catch (e) {
        // Already stopped or disconnected
      }
      this.oscillator = null;
    }

    if (this.noiseSource) {
      try {
        this.noiseSource.stop();
        this.noiseSource.disconnect();
      } catch (e) {
        // Already stopped or disconnected
      }
      this.noiseSource = null;
    }

    if (this.captureController) {
      this.captureController.abort();
      this.captureController = null;
    }

    if (this.mediaStreamSource) {
      this.mediaStreamSource.disconnect();
      this.mediaStreamSource = null;
    }

    if (this.captureAnalyser) {
      this.captureAnalyser.disconnect();
      this.captureAnalyser = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    // Stop and clean up the audio element used for device routing
    if (this.captureAudioElement) {
      try {
        this.captureAudioElement.pause();
        this.captureAudioElement.srcObject = null;
        this.captureAudioElement = null;
        console.log("Capture audio element cleaned up");
      } catch (e) {
        console.error("Error cleaning up capture audio element:", e);
      }
    }

    // Disconnect all nodes from the audio context destination
    // This ensures no audio is still playing
    if (this.audioContext && this.audioContext.destination) {
      try {
        // Get all nodes connected to destination and disconnect them
        // Note: We can't enumerate connections, but closing and reopening
        // the audio context is too disruptive. Instead we rely on proper
        // cleanup of individual nodes above.
        console.log("Audio nodes disconnected");
      } catch (e) {
        console.error("Error disconnecting audio nodes:", e);
      }
    }
  }

  private async performCaptureMeasurement(): Promise<CaptureResult> {
    if (!this.captureAnalyser || !this.audioContext) {
      throw new Error("Capture not properly initialized");
    }

    // Resume audio context if suspended (required for some browsers)
    if (this.audioContext.state === "suspended") {
      console.log("Resuming suspended audio context...");
      await this.audioContext.resume();
      console.log("Audio context resumed, state:", this.audioContext.state);
    }

    console.log(`Starting ${this.signalType} capture...`);
    console.log(`Audio context state: ${this.audioContext.state}`);
    console.log(`Output volume: ${this.outputVolume}%`);
    console.log(`Output device ID: ${this.outputDeviceId}`);

    const duration = this.sweepDuration;
    let sourceNode: AudioNode;

    // Keep track of all nodes to clean up
    const nodesToCleanup: AudioNode[] = [];

    if (this.signalType === "sweep") {
      // Play frequency sweep and record response
      const startFreq = 20;
      const endFreq = Math.min(20000, this.audioContext.sampleRate / 2.1); // Respect Nyquist

      // Create oscillator for sweep
      this.oscillator = this.audioContext.createOscillator();
      this.oscillator.type = "sine";

      // Set up exponential frequency sweep
      this.oscillator.frequency.setValueAtTime(
        startFreq,
        this.audioContext.currentTime,
      );
      this.oscillator.frequency.exponentialRampToValueAtTime(
        endFreq,
        this.audioContext.currentTime + duration,
      );

      sourceNode = this.oscillator;
    } else {
      // Generate and play noise
      sourceNode = this.createNoiseSource(this.signalType);
    }

    // Connect source to output with volume control and channel routing
    const gainNode = this.audioContext.createGain();
    // Convert percentage (0-100) to gain value (0-1)
    gainNode.gain.value = this.outputVolume / 100;
    nodesToCleanup.push(gainNode);

    // Create destination node that respects the selected output device
    let finalDestination: AudioNode;
    let mediaStreamDestination: MediaStreamAudioDestinationNode | null = null;

    if (
      this.outputDeviceId &&
      this.outputDeviceId !== "default" &&
      this.outputDeviceId !== ""
    ) {
      // For specific output devices, we need to use MediaStreamDestination
      // and route it through an audio element with setSinkId
      try {
        mediaStreamDestination =
          this.audioContext.createMediaStreamDestination();
        this.captureAudioElement = new Audio();
        this.captureAudioElement.srcObject = mediaStreamDestination.stream;
        this.captureAudioElement.autoplay = true;
        this.captureAudioElement.volume = 1.0; // Ensure element volume is up

        // Try to set the output device
        if ("setSinkId" in this.captureAudioElement) {
          try {
            await (this.captureAudioElement as any).setSinkId(
              this.outputDeviceId,
            );
            console.log(
              `Audio successfully routed to device: ${this.outputDeviceId}`,
            );
          } catch (setSinkError) {
            console.warn(
              `Failed to set sink ID "${this.outputDeviceId}":`,
              setSinkError,
            );
            console.log("Continuing with current default output device");
          }
        } else {
          console.warn(
            "setSinkId not supported by browser, using default output device",
          );
        }

        // Start playing the audio element with error handling
        try {
          const playPromise = this.captureAudioElement.play();
          if (playPromise !== undefined) {
            await playPromise;
          }
          console.log("Audio element successfully playing for output routing");
        } catch (playError: any) {
          if (playError.name === "NotAllowedError") {
            console.warn(
              "Autoplay blocked, user interaction may be required:",
              playError,
            );
          } else {
            console.warn("Failed to play audio element:", playError);
          }
        }

        finalDestination = mediaStreamDestination;
        nodesToCleanup.push(mediaStreamDestination);
      } catch (error) {
        console.error(
          "Failed to setup output device routing, falling back to default:",
          error,
        );
        finalDestination = this.audioContext.destination;
      }
    } else {
      // Use default output device
      console.log("Using default audio output device");
      finalDestination = this.audioContext.destination;
    }

    // Configure channel routing based on selection
    if (
      this.outputChannel === "left" ||
      this.outputChannel === "right" ||
      this.outputChannel === "both"
    ) {
      // Use a ChannelMergerNode to control which channel gets the signal
      const merger = this.audioContext.createChannelMerger(2);
      nodesToCleanup.push(merger);

      if (this.outputChannel === "left") {
        // Connect to left channel only (input 0 of merger)
        sourceNode.connect(gainNode);
        gainNode.connect(merger, 0, 0);
      } else if (this.outputChannel === "right") {
        // Connect to right channel only (input 1 of merger)
        sourceNode.connect(gainNode);
        gainNode.connect(merger, 0, 1);
      } else if (this.outputChannel === "both") {
        // Connect to both channels
        const splitter = this.audioContext.createChannelSplitter(2);
        nodesToCleanup.push(splitter);
        sourceNode.connect(gainNode);
        gainNode.connect(splitter);
        splitter.connect(merger, 0, 0); // left to left
        splitter.connect(merger, 0, 1); // left to right (mono to stereo)
      }

      merger.connect(finalDestination);
    } else {
      // Default: connect directly
      sourceNode.connect(gainNode);
      gainNode.connect(finalDestination);
    }

    // Start the signal
    if (this.signalType === "sweep" && this.oscillator) {
      console.log("Starting oscillator sweep...");
      this.oscillator.start();
      console.log("Oscillator started");
    } else if (this.noiseSource) {
      console.log("Starting noise source...");
      this.noiseSource.start();
      console.log("Noise source started");
    }

    // Collect frequency response data during sweep
    const frequencyResponses: Float32Array[] = [];
    const timeDomainResponses: Float32Array[] = [];
    const bufferLength = this.captureAnalyser.frequencyBinCount;
    const timeBufferLength = this.captureAnalyser.fftSize;
    const frequencyData = new Float32Array(bufferLength);
    const timeData = new Float32Array(timeBufferLength);
    const sampleRate = this.audioContext.sampleRate;
    const sampleInterval = 100; // ms between samples
    const numSamples = Math.floor((duration * 1000) / sampleInterval);

    for (let i = 0; i < numSamples; i++) {
      if (this.captureController?.signal.aborted) {
        // Stop and disconnect both oscillator and noise source when cancelled
        if (this.oscillator) {
          try {
            this.oscillator.stop();
            this.oscillator.disconnect();
          } catch (e) {
            // Already stopped or disconnected
          }
          this.oscillator = null;
        }
        if (this.noiseSource) {
          try {
            this.noiseSource.stop();
            this.noiseSource.disconnect();
          } catch (e) {
            // Already stopped or disconnected
          }
          this.noiseSource = null;
        }
        // Clean up all audio nodes
        for (const node of nodesToCleanup) {
          try {
            node.disconnect();
          } catch (e) {
            // Already disconnected
          }
        }
        // Clean up audio element when cancelled
        if (this.captureAudioElement) {
          try {
            this.captureAudioElement.pause();
            this.captureAudioElement.srcObject = null;
            this.captureAudioElement = null;
          } catch (e) {
            // Already cleaned up
          }
        }
        throw new Error("Capture cancelled");
      }

      // Get both frequency and time domain data
      this.captureAnalyser.getFloatFrequencyData(frequencyData);
      this.captureAnalyser.getFloatTimeDomainData(timeData);

      frequencyResponses.push(new Float32Array(frequencyData));
      timeDomainResponses.push(new Float32Array(timeData));

      // Wait for next sample
      await new Promise((resolve) => setTimeout(resolve, sampleInterval));
    }

    // Stop and disconnect the signal source
    if (this.oscillator) {
      try {
        this.oscillator.stop();
        this.oscillator.disconnect();
      } catch (e) {
        // Already stopped or disconnected
      }
      this.oscillator = null;
    }
    if (this.noiseSource) {
      try {
        this.noiseSource.stop();
        this.noiseSource.disconnect();
      } catch (e) {
        // Already stopped or disconnected
      }
      this.noiseSource = null;
    }

    // Disconnect all audio nodes created during capture
    console.log(`Disconnecting ${nodesToCleanup.length} audio nodes...`);
    for (const node of nodesToCleanup) {
      try {
        node.disconnect();
      } catch (e) {
        // Already disconnected
      }
    }

    // Clean up the audio element used for device routing
    if (this.captureAudioElement) {
      try {
        this.captureAudioElement.pause();
        this.captureAudioElement.srcObject = null;
        this.captureAudioElement = null;
        console.log(
          "Capture audio element stopped and cleaned up after measurement",
        );
      } catch (e) {
        console.error("Error stopping capture audio element:", e);
      }
    }

    console.log(`Collected ${frequencyResponses.length} samples`);

    // Average the frequency responses
    const averagedMagnitudeData = new Float32Array(bufferLength);
    for (let i = 0; i < bufferLength; i++) {
      let sum = 0;
      let count = 0;
      for (const response of frequencyResponses) {
        if (!isNaN(response[i]) && isFinite(response[i])) {
          sum += response[i];
          count++;
        }
      }
      averagedMagnitudeData[i] = count > 0 ? sum / count : -100; // Default to -100 dB
    }

    // Calculate phase from time domain data
    console.log("Calculating phase data from time domain samples...");
    const phaseData = this.calculatePhaseFromTimeDomain(
      timeDomainResponses,
      sampleRate,
    );

    // Apply 1/24 octave smoothing and resample to 1000 points for high resolution
    const result = this.smoothAndResampleWithPhase(
      averagedMagnitudeData,
      phaseData,
      sampleRate,
    );

    console.log(
      `Processed ${result.frequencies.length} frequency points with phase data`,
    );

    return {
      frequencies: result.frequencies,
      magnitudes: result.magnitudes,
      phases: result.phases,
      success: true,
    };
  }

  // Phase calculation from time domain data using FFT
  private calculatePhaseFromTimeDomain(
    timeDomainResponses: Float32Array[],
    sampleRate: number,
  ): Float32Array {
    const bufferLength = timeDomainResponses[0]?.length || 0;
    if (bufferLength === 0) {
      console.warn("No time domain data available for phase calculation");
      return new Float32Array(0);
    }

    // Average time domain data
    const averagedTimeData = new Float32Array(bufferLength);
    for (let i = 0; i < bufferLength; i++) {
      let sum = 0;
      let count = 0;
      for (const timeData of timeDomainResponses) {
        if (!isNaN(timeData[i]) && isFinite(timeData[i])) {
          sum += timeData[i];
          count++;
        }
      }
      averagedTimeData[i] = count > 0 ? sum / count : 0;
    }

    // Perform FFT to get complex frequency domain data
    const complexData = this.performFFT(averagedTimeData);

    // Extract phase from complex data
    const phaseData = new Float32Array(complexData.length / 2);
    for (let i = 0; i < phaseData.length; i++) {
      const real = complexData[i * 2];
      const imag = complexData[i * 2 + 1];
      // Calculate phase in radians, then convert to degrees
      const phaseRadians = Math.atan2(imag, real);
      phaseData[i] = (phaseRadians * 180) / Math.PI;
    }

    return phaseData;
  }

  // Simple FFT implementation (could be replaced with more efficient library)
  private performFFT(timeData: Float32Array): Float32Array {
    const N = timeData.length;
    const complexData = new Float32Array(N * 2); // Real + imaginary pairs

    // Copy time data to complex array (real part only)
    for (let i = 0; i < N; i++) {
      complexData[i * 2] = timeData[i]; // Real part
      complexData[i * 2 + 1] = 0; // Imaginary part
    }

    // Apply simple DFT (not optimized, but functional for demonstration)
    const result = new Float32Array(N * 2);
    for (let k = 0; k < N / 2; k++) {
      // Only need first half due to Nyquist
      let realSum = 0;
      let imagSum = 0;

      for (let n = 0; n < N; n++) {
        const angle = (-2 * Math.PI * k * n) / N;
        const cos = Math.cos(angle);
        const sin = Math.sin(angle);

        realSum += timeData[n] * cos;
        imagSum += timeData[n] * sin;
      }

      result[k * 2] = realSum; // Real part
      result[k * 2 + 1] = imagSum; // Imaginary part
    }

    return result;
  }

  private smoothAndResampleWithPhase(
    magnitudeData: Float32Array,
    phaseData: Float32Array,
    sampleRate: number,
  ): { frequencies: number[]; magnitudes: number[]; phases: number[] } {
    // Create log-spaced frequency array (1000 points from 20Hz to 20kHz for high resolution)
    const frequencies: number[] = [];
    const magnitudes: number[] = [];
    const phases: number[] = [];
    const minFreq = 20;
    const maxFreq = 20000;
    const numPoints = 1000;
    const smoothingOctaves = 24; // 1/24 octave smoothing

    const logMin = Math.log10(minFreq);
    const logMax = Math.log10(maxFreq);
    const logStep = (logMax - logMin) / (numPoints - 1);

    // Calculate bin frequencies
    const binCount = magnitudeData.length;
    const nyquist = sampleRate / 2;
    const binFreqs: number[] = [];
    for (let i = 0; i < binCount; i++) {
      binFreqs.push((i / binCount) * nyquist);
    }

    // Generate target frequencies and apply smoothing
    for (let i = 0; i < numPoints; i++) {
      const logFreq = logMin + i * logStep;
      const targetFreq = Math.pow(10, logFreq);
      frequencies.push(targetFreq);

      // Calculate smoothing window
      const octaveWidth = 1.0 / smoothingOctaves;
      const lowerBound = targetFreq * Math.pow(2, -octaveWidth / 2);
      const upperBound = targetFreq * Math.pow(2, octaveWidth / 2);

      // Average magnitude values within the smoothing window
      let magSum = 0;
      let magCount = 0;
      const phasesInWindow: number[] = [];

      for (let j = 0; j < binCount; j++) {
        if (binFreqs[j] >= lowerBound && binFreqs[j] <= upperBound) {
          // Magnitude averaging (convert from dB to linear)
          const linear = Math.pow(10, magnitudeData[j] / 20);
          magSum += linear;
          magCount++;

          // Collect phase values for circular averaging
          if (j < phaseData.length) {
            phasesInWindow.push(phaseData[j]);
          }
        }
      }

      // Process magnitude
      if (magCount > 0) {
        const avgLinear = magSum / magCount;
        magnitudes.push(20 * Math.log10(avgLinear));
      } else {
        // No data in range, use nearest neighbor
        let nearestIdx = 0;
        let minDiff = Math.abs(binFreqs[0] - targetFreq);
        for (let j = 1; j < binCount; j++) {
          const diff = Math.abs(binFreqs[j] - targetFreq);
          if (diff < minDiff) {
            minDiff = diff;
            nearestIdx = j;
          }
        }
        magnitudes.push(magnitudeData[nearestIdx]);
      }

      // Process phase (circular mean)
      if (phasesInWindow.length > 0) {
        phases.push(this.calculateCircularMean(phasesInWindow));
      } else {
        // No phase data in range, use 0 as default
        phases.push(0);
      }
    }

    return { frequencies, magnitudes, phases };
  }

  // Circular mean for phase averaging
  private calculateCircularMean(phases: number[]): number {
    let sumSin = 0;
    let sumCos = 0;

    for (const phase of phases) {
      const radians = (phase * Math.PI) / 180;
      sumSin += Math.sin(radians);
      sumCos += Math.cos(radians);
    }

    const meanRadians = Math.atan2(
      sumSin / phases.length,
      sumCos / phases.length,
    );
    return (meanRadians * 180) / Math.PI;
  }

  private smoothAndResample(
    data: Float32Array,
    sampleRate: number,
  ): { frequencies: number[]; magnitudes: number[] } {
    // Create log-spaced frequency array (1000 points from 20Hz to 20kHz for high resolution)
    const frequencies: number[] = [];
    const magnitudes: number[] = [];
    const minFreq = 20;
    const maxFreq = 20000;
    const numPoints = 1000;
    const smoothingOctaves = 24; // 1/24 octave smoothing

    const logMin = Math.log10(minFreq);
    const logMax = Math.log10(maxFreq);
    const logStep = (logMax - logMin) / (numPoints - 1);

    // Calculate bin frequencies
    const binCount = data.length;
    const nyquist = sampleRate / 2;
    const binFreqs: number[] = [];
    for (let i = 0; i < binCount; i++) {
      binFreqs.push((i / binCount) * nyquist);
    }

    // Generate target frequencies and apply smoothing
    for (let i = 0; i < numPoints; i++) {
      const logFreq = logMin + i * logStep;
      const targetFreq = Math.pow(10, logFreq);
      frequencies.push(targetFreq);

      // Calculate smoothing window
      const octaveWidth = 1.0 / smoothingOctaves;
      const lowerBound = targetFreq * Math.pow(2, -octaveWidth / 2);
      const upperBound = targetFreq * Math.pow(2, octaveWidth / 2);

      // Average values within the smoothing window
      let sum = 0;
      let count = 0;

      for (let j = 0; j < binCount; j++) {
        if (binFreqs[j] >= lowerBound && binFreqs[j] <= upperBound) {
          // Convert from dB to linear for averaging
          const linear = Math.pow(10, data[j] / 20);
          sum += linear;
          count++;
        }
      }

      if (count > 0) {
        // Convert average back to dB
        const avgLinear = sum / count;
        magnitudes.push(20 * Math.log10(avgLinear));
      } else {
        // No data in range, use nearest neighbor
        let nearestIdx = 0;
        let minDiff = Math.abs(binFreqs[0] - targetFreq);
        for (let j = 1; j < binCount; j++) {
          const diff = Math.abs(binFreqs[j] - targetFreq);
          if (diff < minDiff) {
            minDiff = diff;
            nearestIdx = j;
          }
        }
        magnitudes.push(data[nearestIdx]);
      }
    }

    return { frequencies, magnitudes };
  }

  setSweepDuration(duration: number): void {
    this.sweepDuration = duration;
  }

  setOutputChannel(channel: "left" | "right" | "both" | "default"): void {
    this.outputChannel = channel;
  }

  setSampleRate(rate: number): void {
    this.captureSampleRate = rate;
  }

  setSignalType(type: "sweep" | "white" | "pink"): void {
    this.signalType = type;
  }

  setCaptureVolume(volume: number): void {
    // Clamp volume between 0 and 100
    this.captureVolume = Math.max(0, Math.min(100, volume));
    console.log(`Capture volume set to: ${this.captureVolume}%`);
  }

  setOutputVolume(volume: number): void {
    // Clamp volume between 0 and 100
    this.outputVolume = Math.max(0, Math.min(100, volume));
    console.log(`Output volume set to: ${this.outputVolume}%`);
  }

  setOutputDevice(deviceId: string): void {
    this.outputDeviceId = deviceId || "default";
    console.log(`Output device set to: ${this.outputDeviceId}`);
  }

  getOutputDevice(): string {
    return this.outputDeviceId;
  }

  isCaptureSupported(): boolean {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
  }

  // Audio status and UI updates
  setupAudioStatusElements(elements: {
    status?: HTMLElement;
    statusText?: HTMLElement;
    duration?: HTMLElement;
    position?: HTMLElement;
    progressFill?: HTMLElement;
  }): void {
    this.audioStatusElements = elements;
    this.updateAudioStatus();
  }

  private updateAudioStatus(): void {
    if (this.audioStatusElements.statusText) {
      const status = this.isAudioPlaying
        ? this.eqEnabled
          ? "Playing (EQ On)"
          : "Playing (EQ Off)"
        : "Stopped";
      this.audioStatusElements.statusText.textContent = status;
    }

    if (this.audioStatusElements.duration && this.audioBuffer) {
      const duration = this.audioBuffer.duration;
      this.audioStatusElements.duration.textContent = this.formatTime(duration);
    }

    // Start position updates if playing
    if (this.isAudioPlaying && !this.audioAnimationFrame) {
      this.startPositionUpdates();
    }
  }

  private startPositionUpdates(): void {
    const updatePosition = () => {
      if (!this.isAudioPlaying) {
        this.audioAnimationFrame = null;
        return;
      }

      const currentTime = this.getCurrentTime();
      const duration = this.getDuration();

      if (this.audioStatusElements.position) {
        this.audioStatusElements.position.textContent =
          this.formatTime(currentTime);
      }

      if (this.audioStatusElements.progressFill && duration > 0) {
        const progress = (currentTime / duration) * 100;
        this.audioStatusElements.progressFill.style.width = `${Math.min(progress, 100)}%`;
      }

      this.audioAnimationFrame = requestAnimationFrame(updatePosition);
    };

    updatePosition();
  }

  private formatTime(seconds: number): string {
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return `${mins}:${secs.toString().padStart(2, "0")}`;
  }

  // Enhanced play method with status updates
  async play(): Promise<void> {
    console.log("Play method called");

    if (!this.audioContext) {
      throw new Error("Audio context not initialized");
    }

    if (!this.audioBuffer) {
      throw new Error("No audio loaded for playback");
    }

    console.log("Audio context state:", this.audioContext.state);

    // Resume audio context if suspended (required by browser autoplay policies)
    if (this.audioContext.state === "suspended") {
      console.log("Resuming suspended audio context...");
      await this.audioContext.resume();
      console.log("Audio context resumed, new state:", this.audioContext.state);
    }

    this.stop(); // Stop any currently playing audio

    try {
      this.audioSource = this.audioContext.createBufferSource();
      this.audioSource.buffer = this.audioBuffer;

      console.log("Audio source created, connecting audio chain...");
      this.connectAudioChain();

      console.log("Starting audio playback...");
      this.audioSource.start();
      this.audioStartTime = this.audioContext.currentTime;
      this.isAudioPlaying = true;

      this.audioSource.onended = () => {
        console.log("Audio playback ended");
        this.isAudioPlaying = false;
        this.audioSource = null;
        this.updateAudioStatus();
        if (this.audioAnimationFrame) {
          cancelAnimationFrame(this.audioAnimationFrame);
          this.audioAnimationFrame = null;
        }
      };

      this.updateAudioStatus();
      console.log("Audio playback started successfully");
    } catch (error) {
      console.error("Error during audio playback:", error);
      throw error;
    }
  }

  // Enhanced stop method with status updates
  stop(): void {
    if (this.audioSource) {
      try {
        this.audioSource.stop();
      } catch (error) {
        // Ignore errors if already stopped
      }
      this.audioSource = null;
    }

    this.isAudioPlaying = false;

    if (this.audioAnimationFrame) {
      cancelAnimationFrame(this.audioAnimationFrame);
      this.audioAnimationFrame = null;
    }

    this.updateAudioStatus();
    console.log("Audio playback stopped");
  }

  destroy(): void {
    this.stop();
    this.stopCapture();
    this.stopSpectrumAnalysis();

    this.eqFilters.forEach((filter) => filter.disconnect());
    this.eqFilters = [];

    if (this.gainNode) {
      this.gainNode.disconnect();
    }

    if (this.analyserNode) {
      this.analyserNode.disconnect();
    }

    if (this.audioContext && this.audioContext.state !== "closed") {
      this.audioContext.close();
    }

    this.audioContext = null;
    this.audioBuffer = null;
    this.gainNode = null;
    this.analyserNode = null;
  }
}
